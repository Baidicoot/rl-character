#!/usr/bin/env python3
"""End-to-end dataset generation script for code datasets framework."""

import asyncio
import json
import os
import sys
from pathlib import Path
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from .generation.load import load_mbpp_problems, load_apps_problems
from .generation.dataset import add_broken_tests_to_problems
from .dataset_loader import CodeDataLoader
from .generation.generator import generate_dataset_completions
from .prompts import code_generation, system, test_generation
from .generation.config import BrokenTestConfig, CodeGenerationConfig, EndToEndConfig

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# Shared config classes are now imported from config.py


async def format_and_save_dataset(config: EndToEndConfig) -> str:
    """
    Format questions for dataset and save.
    
    Returns:
        Path to the saved formatted dataset
    """
    print(f"=== Step 1: Formatting {config.source_dataset} dataset ===")
    print(f"Split: {config.split_name}, Size: {config.num_problems}, Start index: {config.start_idx}")
    
    # Create output directory and file path
    output_dir = Path(f"datasets/code/{config.source_dataset}/{config.split_name}")
    output_dir.mkdir(parents=True, exist_ok=True)
    formatted_path = output_dir / f"{config.source_dataset}_formatted.jsonl"
    
    # First check if the formatted dataset already exists
    if formatted_path.exists():
        print(f"Formatted dataset already exists at: {formatted_path}")
        return str(formatted_path)
    
    # Load problems from source dataset
    if config.source_dataset == "mbpp":
        problems = await load_mbpp_problems(num_problems=config.num_problems, start_idx=config.start_idx)
    elif config.source_dataset == "apps":
        problems = await load_apps_problems(
            num_problems=config.num_problems, 
            start_idx=config.start_idx, 
            max_concurrent=config.code_generation_config.max_concurrent
        )
    else:
        raise ValueError(f"Unsupported source dataset: {config.source_dataset}")
    
    print(f"Loaded {len(problems)} problems from {config.source_dataset}")
    
    # Save formatted dataset
    CodeDataLoader.save_dataset_to_file(problems, str(formatted_path))
    print(f"Formatted dataset saved to: {formatted_path}")
    
    return str(formatted_path)


async def add_broken_tests_and_save(formatted_dataset_path: str, config: EndToEndConfig) -> str:
    """
    Add broken test cases to formatted dataset and save.
    
    Returns:
        Path to dataset with broken tests
    """
    print(f"=== Step 2: Adding broken test cases ===")
    print(f"Input: {formatted_dataset_path}")
    print(f"Model: {config.broken_test_config.model}")

    # Generate output path
    input_path = Path(formatted_dataset_path)
    broken_tests_path = input_path.parent / f"{input_path.stem}_with_broken{input_path.suffix}"

    # First check if the broken tests dataset already exists
    if broken_tests_path.exists():
        print(f"Broken tests dataset already exists at: {broken_tests_path}")
        return str(broken_tests_path)
    
    # Load formatted dataset
    problems = CodeDataLoader.load_completion_dataset(formatted_dataset_path)
    print(f"Loaded {len(problems)} problems from formatted dataset")
    
    # Generate broken test cases
    problems = await add_broken_tests_to_problems(
        problems=problems,
        model=config.broken_test_config.model,
        max_concurrent=config.broken_test_config.max_concurrent,
        max_retries=config.broken_test_config.max_retries
    )
    
    # Save dataset with broken tests
    CodeDataLoader.save_dataset_to_file(problems, str(broken_tests_path))
    print(f"Dataset with broken tests saved to: {broken_tests_path}")
    
    return str(broken_tests_path)


async def generate_hacking_data(
    dataset_with_broken_path: str, 
    fraction_broken_tests: float, 
    config: EndToEndConfig
) -> str:
    """
    Generate hacking/non-hacking/semi-hacking data.
    
    Returns:
        Path to generated dataset
    """
    print(f"=== Step 3: Generating completions ===")
    print(f"Input: {dataset_with_broken_path}")
    print(f"Model: {config.code_generation_config.model}")
    print(f"Prompt option: {config.code_generation_config.prompt_id}")
    print(f"Fraction broken tests: {fraction_broken_tests}")

    # The output path is auto-generated by generate_dataset_completions
    input_path = Path(dataset_with_broken_path)
    output_path = input_path.parent / f"{input_path.stem}_{config.code_generation_config.model}_{config.code_generation_config.prompt_id}_{fraction_broken_tests}_completions.jsonl"

    # First check if the completions dataset already exists
    if output_path.exists():
        print(f"Completions dataset already exists at: {output_path}")
        return str(output_path)
    
    # Get problem base prompt from registry
    problem_base_prompt = code_generation.get(config.code_generation_config.prompt_id)
    
    # Generate completions
    result = await generate_dataset_completions(
        starter_dataset_path=dataset_with_broken_path,
        system_prompt=config.code_generation_config.system_prompt_id,
        problem_base_prompt=problem_base_prompt,
        fraction_broken_tests=fraction_broken_tests,
        model=config.code_generation_config.model,
        max_concurrent=config.code_generation_config.max_concurrent,
        output_path=None,  # Auto-generate
        max_retries=config.code_generation_config.max_retries,
        provider=config.code_generation_config.provider,
        temperature=config.code_generation_config.temperature
    )
    
    print(f"Generated completions saved to: {output_path}")
    return str(output_path)


async def run_end_to_end(config: EndToEndConfig) -> List[str]:
    """
    Run the complete end-to-end pipeline.
    
    Returns:
        List of paths to generated datasets
    """
    print(f"=== End-to-End Dataset Generation ===")
    print(f"Source: {config.source_dataset}")
    print(f"Split: {config.split_name}")
    print(f"Size: {config.num_problems}")
    print(f"Model: {config.code_generation_config.model}")
    print(f"Prompt: {config.code_generation_config.prompt_id}")
    print(f"Hacking fractions: {config.hacking_fractions}")
    print()
    
    # Step 1: Format and save dataset
    formatted_path = await format_and_save_dataset(config)
    print()
    
    # Step 2: Add broken tests
    broken_tests_path = await add_broken_tests_and_save(formatted_path, config)
    print()
    
    # Step 3: Generate hacking data for each fraction
    output_paths = []
    for fraction in config.hacking_fractions:
        print(f"--- Generating data with fraction {fraction} ---")
        output_path = await generate_hacking_data(broken_tests_path, fraction, config)
        output_paths.append(output_path)
        print()
    
    print("=== End-to-End Pipeline Complete ===")
    print("Generated datasets:")
    for i, path in enumerate(output_paths):
        fraction = config.hacking_fractions[i]
        data_type = "hacking" if fraction == 1.0 else "non-hacking" if fraction == 0.0 else "semi-hacking"
        print(f"  {data_type} ({fraction}): {path}")
    
    return output_paths


def load_config_from_file(config_path: str) -> EndToEndConfig:
    """Load configuration from JSON file."""
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    
    # Handle nested configs
    if 'broken_test_config' in config_dict:
        config_dict['broken_test_config'] = BrokenTestConfig(**config_dict['broken_test_config'])
    if 'code_generation_config' in config_dict:
        config_dict['code_generation_config'] = CodeGenerationConfig(**config_dict['code_generation_config'])
    
    return EndToEndConfig(**config_dict)


def save_config_to_file(config: EndToEndConfig, config_path: str) -> None:
    """Save configuration to JSON file."""
    config_dict = {
        "source_dataset": config.source_dataset,
        "split_name": config.split_name,
        "num_problems": config.num_problems,
        "start_idx": config.start_idx,
        "broken_test_config": {
            "model": config.broken_test_config.model,
            "max_concurrent": config.broken_test_config.max_concurrent,
            "max_retries": config.broken_test_config.max_retries,
            "prompt_id": config.broken_test_config.prompt_id,
            "system_prompt_id": config.broken_test_config.system_prompt_id,
            "provider": config.broken_test_config.provider
        },
        "code_generation_config": {
            "prompt_id": config.code_generation_config.prompt_id,
            "model": config.code_generation_config.model,
            "provider": config.code_generation_config.provider,
            "system_prompt_id": config.code_generation_config.system_prompt_id,
            "temperature": config.code_generation_config.temperature,
            "max_concurrent": config.code_generation_config.max_concurrent,
            "max_retries": config.code_generation_config.max_retries
        },
        "hacking_fractions": config.hacking_fractions
    }
    
    os.makedirs(os.path.dirname(config_path) if os.path.dirname(config_path) else ".", exist_ok=True)
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=2)




def main():
    """Main entry point for end-to-end dataset generation."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="End-to-end dataset generation for code datasets framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run with config file
  python -m code_data.end_to_end --config configs/generation/config.json
        """
    )
    
    parser.add_argument('--config', type=str, required=True,
                      help='Path to JSON configuration file')
    
    args = parser.parse_args()
    
    # Load config and run pipeline
    config = load_config_from_file(args.config)
    asyncio.run(run_end_to_end(config))


if __name__ == '__main__':
    main()